{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 11 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (5+5+10+5+5 points)\n",
    "\n",
    "(From Strang, section 6.4, problem 18.)\n",
    "\n",
    "Let $A$ be some rectangular (possibly non-square, possibly complex) matrix.  Form the block matrix\n",
    "$$S = \\begin{pmatrix} 0 & A \\\\ A^H & 0 \\end{pmatrix} $$\n",
    "where \"0\" denotes a block of zeros of the appropriate size. Consider the eigenvalues λ and eigenvectors $x = (y,z)$ of S, satisfying $Sx = \\lambda x$:\n",
    "\n",
    "$$\n",
    "Sx = \\underbrace{\\begin{pmatrix} 0 & A \\\\ A^H & 0 \\end{pmatrix}}_S \\underbrace{\\begin{pmatrix} y \\\\ z \\end{pmatrix}}_x = \\lambda \\begin{pmatrix} y \\\\ z \\end{pmatrix} = \\lambda x\n",
    "$$\n",
    "\n",
    "**(a)** The eigenvalues of $S$ must be *real* because ...............\n",
    "\n",
    "**(b)** If $A$ is $m\\times n$, how big are the vectors $y$ and $z$, and how big are the two blocks of 0's in $S$?\n",
    "\n",
    "**(c)** Show that $-\\lambda$ is also an eigenvalue, with eigenvector $(y,-z)$.\n",
    "\n",
    "Check this for a random $3\\times 4$ matrix `A = rand(Complex{Float64},3,4)`, with `S = [ 0I A; A' 0I]`.  Compute `eigvals(S)`: does it match your prediction?\n",
    "\n",
    "**(d)** Show (for the same $z$) that $A^HAz = \\lambda^2 z$, so that $\\lambda^2$ is an eigenvalue of $A^H A$.  Check this via `eigvals(A'*A)`.\n",
    "\n",
    "**(e)** If $A = I$ ($2 \\times 2$), find all four eigenvectors and eigenvalues of $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)**  $\\boxed{S \\text{ is Hermitian.}}$\n",
    "\n",
    "**(b)**  For the product of block matrices to make sense we need that the products $A^Ty$ and $Az$ should make sense.  Since $A$ is $m \\times n$ and therefore $A^T$ is $n \\times m$, this means that $\\boxed{y \\text{ is } n \\times 1}$ and $\\boxed{z \\text{ is } m \\times 1}$.  The upper left block of zeros is as tall as $A$, i.e. height $m$, and as wide as $A^T$, i.e. width m, so $\\boxed{\\text{ upper left block of zeros is } m \\times m.}$  Similarly, the $\\boxed{\\text{ lower right block of zeros is } n \\times n.}$\n",
    "\n",
    "**(c)**  Let's first unravel what it means for $(y, z)$ to be an eigenvector of $S$ with eigenvalue $\\lambda$.  Doing the block matrix multiplication, we get $$S(y, z) = \\begin{pmatrix} 0 & A \\\\ A^H & 0\\end{pmatrix}\\begin{pmatrix}y \\\\ z\\end{pmatrix} = \\begin{pmatrix} Az \\\\ A^Hy\\end{pmatrix}.$$ So, that this equals $\\lambda(y, z)$ means that $Az = \\lambda y$ and $A^Hy = \\lambda z$.  So, we get $$S(y, -z) = \\begin{pmatrix} 0 & A \\\\ A^H & 0\\end{pmatrix}\\begin{pmatrix}y \\\\ -z\\end{pmatrix} = \\begin{pmatrix} -Az \\\\ A^Hy\\end{pmatrix} = \\begin{pmatrix}-\\lambda y \\\\ \\lambda z \\end{pmatrix} = -\\lambda(y, -z).$$ So, $-\\lambda$ is also an eigenvector for $S$ with a corresponding eigenvector $(y, -z)$.\n",
    "\n",
    "**(d)**  Using the facts $A^Hy = \\lambda z$ and $Az = \\lambda y$ that we worked out above, we get $$\\boxed{A^HAz = A^H(Az) = A^H\\lambda y = \\lambda A^Hy = \\lambda^2z}$$ so indeed $\\lambda^2$ is an eigenvalue of $A^HA$ with eigenvalue $\\lambda$.  The Julia code below shows that the eigenvalues of $A^HA$ for a random $A$ are indeed the squares of those of $S$, ignoring multiplicity (notice that the very small eigenvalue doesn't do so well numerically, but it's basically zero in each case).\n",
    "\n",
    "**(e)**  When $A = I$ is $2 \\times 2$, $S$ is the $4 \\times 4$ matrix that swaps the first and third entries and the second and fourth entries of a $4 \\times 1$ column vector.  So any nonzero $4 \\times 1$ column vector of the form $(x, x)$ for some $x \\in \\mathbb{R}^2$ will be an eigenvector with $\\boxed{\\text{eigenvalue } 1}$, and that these are precisely all such eigenvectors; we could choose the basis $\\boxed{(1, 0, 1, 0), (0, 1, 0, 1)}$ for this eigenspace.  So there is a 2-dimensional 1-eigenspace.  From part (c), we see that any vector $(x, -x)$ for $x \\in \\mathbb{R}^2$ will give an eigenvector with $\\boxed{\\text{eigenvalue } -1}$, and these are precisely all such eigenvectors; we could choose the basis $\\boxed{(1, 0, -1, 0), (0, 1, 0, -1)}$ for this eigenspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{Float64,1}:\n",
       " -2.74904    \n",
       " -0.91782    \n",
       " -0.232195   \n",
       " -7.22541e-19\n",
       "  0.232195   \n",
       "  0.91782    \n",
       "  2.74904    "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(Complex{Float64},3,4)\n",
    "S = [ 0I A; A' 0I]\n",
    "eigvals(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{Float64,1}:\n",
       " 7.5572     \n",
       " 0.842394   \n",
       " 0.0539144  \n",
       " 5.22066e-37\n",
       " 0.0539144  \n",
       " 0.842394   \n",
       " 7.5572     "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(S).^2  # squares of eigenvalues of S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 1.55226e-16\n",
       " 0.0539144  \n",
       " 0.842394   \n",
       " 7.5572     "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(A'*A)  # matches squares of the eigenvalues of S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (5+10+5+5+5 points)\n",
    "\n",
    "(From Strang, section 6.4, problem 33.)\n",
    "\n",
    "Suppose $A^T = -A$, a real *antisymmetric* matrix (also called *skew-symmetric*).   Form a random real antisymmetric $5\\times 5$ matrix in Julia via `A = randn(5,5); A = A - A'`.\n",
    "\n",
    "Explain the following facts about $A$, *and* check each fact numerically for your random `A` matrix:\n",
    "\n",
    "**(a)** $x^T A x = 0$ for every real vector $x$.  (Try `x'*A*x` in Julia with `x = randn(5)`.)\n",
    "\n",
    "**(b)** The eigenvalues of $A$ (`eigvals(A)`) are **purely imaginary**.  (There are multiple ways to show this, but it is a good excuse to review the proof from class that Hermitian matrices have real eigenvalues… almost the same proof works here.)\n",
    "\n",
    "**(c)** The determinant of $A$ (`det(A)`) is **positive or zero** (not negative).\n",
    "\n",
    "**(d)** The matrix $e^{A}$ is **unitary** (check `Q'*Q - I` for `Q = expm(A)`.)  Why?\n",
    "\n",
    "**(e)** If you solve $dx/dt = Ax$ for any initial condition $x(0)$, then the length of $x$ is conserved: $\\Vert x(t) \\Vert = \\Vert x(0) \\Vert$ for all $t$.    (In Julia, compare `norm(expm(A*t)*x)` to `norm(x)` for various `t`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)**    If $x$ is a real vector such that the multiplication $x^TAx$ makes sense, then $x^TAx$ is itself a real number or a \"$1 \\times 1$ matrix\".  In particular, it's equal to its own transpose.  But using that $A^T = -A$ we get $$(x^TAx)^T = x^TA^Tx = -x^TAx.$$  The only number equal to its negative is $0$, so $x^TAx = 0$, as needed.\n",
    "\n",
    "**(b)**  There are several ways to prove this.\n",
    "\n",
    "One way is just to repeat the proof, from class, that Hermitian matrices have real eigenvalues, and see what happens instead if we have $A^H = -A$ (a real $A^T = -A$ is a special case of this).   If $Ax=\\lambda x$, we just multiplied both sides by $x^H$ in class, and doing the same thing here we obtain:\n",
    "\n",
    "$$\n",
    "x^H A x = \\lambda \\Vert x \\Vert^2 = (A^H x)^H x = (-Ax)^H x = -\\bar{\\lambda} \\Vert x \\Vert^2\n",
    "$$\n",
    "\n",
    "Since $\\Vert x \\Vert^2 \\ne 0$, we get $\\lambda = -\\bar{\\lambda}$ (the same as in class but *with a sign flip*), so λ must be purely imaginary (to equal minus its conjugate).\n",
    "\n",
    "A second approach is even quicker: if $A^H = -A^H$, then $(iA)^H = \\bar{i} A^H = (-i) (-A) = iA$, so $iA$ is **Hermitian**.  That means that $iA$ has *real* eigenvalues, and $A = (-i) (iA)$ therefore has the *same* eigenvalues multiplied by $-i$, which makes them purely imaginary.\n",
    "\n",
    "**(c)**  Recall that if the eigenvalues of a matrix $M$ are $\\lambda_1, ..., \\lambda_m$, then the eigenvalues of the matrix $\\overline{M}$ obtained from $M$ by taking the complex conjugate of every entry are $\\overline{\\lambda_1}, ..., \\overline{\\lambda_m}$ (this is because the characteristic polynomial of $\\overline{M}$ is the complex conjugate of the characteristic polynomial of $M$, so their roots are related by complex conjugates).  So, since the matrix $A$ is real, its eigenvalues (with multiplicity) are stable under complex conjugation, as a collection, i.e. $\\{\\lambda_1, ..., \\lambda_m\\} = \\{\\overline{\\lambda_1}, ..., \\overline{\\lambda_m}\\}$ are the same, counting with multiplicity.  From part (b) we know that the eigenvalues are all purely imaginary, so they are of the form $r_1i, ..., r_mi$.  But then we have $\\{r_1i, ..., r_mi\\} = \\{-r_1i, ..., -r_mi\\}$.  In particular, the eigenvalues are all either 0 or pair off in pairs of the form $ri, -ri$ for some real number $r$; it follows that their product is nonnegative.  As the determinant is the product of the eigenvalues, the claim follows.\n",
    "\n",
    "**(d)**  Since all matrices appearing are real, unitary just means orthogonal.  We need to check that $(e^A)^Te^A = I$.  Notice that for any square matrix $A$, $(e^A)^T = e^{A^T}$ - this follows from the definition of matrix exponential: $$(e^A)^T = \\left(\\sum_{n = 0}^\\infty \\frac{A^n}{n!}\\right)^T = \\sum_{n = 0}^\\infty \\frac{(A^T)^n}{n!} = e^{A^T}.$$  So, since $A$ is anti-symmetric we have $A^T = -A$ and hence $e^{A^T} = e^{-A}$.  We showed in class that $e^{-A}$ is the inverse of $e^A$ (since $A$ and $-A$ *commute*), so we're done: $(e^A)^Te^A = e^{-A} e^A = I$.\n",
    "\n",
    "**(e)**  The solution with initial condition $x(0)$ is $x(t) = e^{tA}x(0)$.  In particular, to check that the length doesn't change we should compute it (or, more easily, its square): $$\\Vert x(t) \\Vert^2 = x(t)^Hx(t) = (e^{tA}x(0))^H(e^{tA}x(0)) = x(0)^H(e^{tA})^He^{tA}x(0) = x(0)^HIx(0) = x(0)^Hx(0) = \\Vert x(0) \\Vert^2$$ (we used part (d) and that $tA$ is also anti-symmetric for any $t \\in \\mathbb{R}$).  In particular, the length of $x(t)$ is constant in time, as needed.\n",
    "\n",
    "Another way of saying this is that we just proved in part (d) that $T = e^{At}$ is unitary, and unitary matrices preserve lengths: $\\Vert Tx \\Vert^2 = (Tx)^H (Tx) = x^H T^H T x = x^H T^{-1} T x = x^H x = \\Vert x \\Vert^2$.\n",
    "\n",
    "All of these things are checked in Julia below, using the recommended code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  0.0       -0.707408     3.2473    -0.779567  -2.25974   \n",
       "  0.707408   0.0          2.47624    1.03915    0.00339821\n",
       " -3.2473    -2.47624      0.0       -0.515485   0.28994   \n",
       "  0.779567  -1.03915      0.515485   0.0       -0.815664  \n",
       "  2.25974   -0.00339821  -0.28994    0.815664   0.0       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = randn(5,5); A = A - A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking part (a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6653345369377348e-16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = randn(5)\n",
    "x'*A*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, it's zero (up to roundoff errors).\n",
    "\n",
    "Checking part (b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Complex{Float64},1}:\n",
       " -3.05311e-16+4.67576im\n",
       " -3.05311e-16-4.67576im\n",
       " -2.28983e-16+1.76733im\n",
       " -2.28983e-16-1.76733im\n",
       "  1.87676e-16+0.0im    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, they are purely imaginary (up to roundoff errors).\n",
    "\n",
    "Checking part (c):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it is 0 or positive.  But this test case isn't entirely satisfying: it is zero because the matrix size is *odd*, so there is always a zero eigenvalue (the other eigenvalues come in conjugate pairs).\n",
    "\n",
    "Let's try an even-size matrix, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.65323391840795"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = randn(6,6); B = B - B'\n",
    "det(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that was positive, as expected.\n",
    "\n",
    "Checking part (c), unitarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  4.44089e-16  -2.53675e-16   1.69932e-16  1.25777e-16  -1.2106e-17 \n",
       " -2.53675e-16   4.44089e-16   3.51765e-17  1.18662e-16  -2.54753e-16\n",
       "  1.69932e-16   3.51765e-17  -1.11022e-16  9.99876e-17   6.43257e-17\n",
       "  1.25777e-16   1.18662e-16   9.99876e-17  2.22045e-16   3.53047e-16\n",
       " -1.2106e-17   -2.54753e-16   6.43257e-17  3.53047e-16   4.44089e-16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = expm(A)\n",
    "Q'*Q - I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, zero up to roundoff errors.\n",
    "\n",
    "Checking part (d), norm preserved by $e^{At}x$, for $t = 5.394828$ and $t = 81.328$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.881784197001252e-16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(expm(A*5.394828)*x) - norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1546319456101628e-14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(expm(A*81.328)*x) - norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray, math works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (5+5+10+10 points)\n",
    "\n",
    "(From Strang, section 6.3, problem 31.)\n",
    "\n",
    "The **cosine of a matrix** can be defined like $e^A$, by copying the [Taylor series for](https://en.wikipedia.org/wiki/Trigonometric_functions#Series_definitions) $\\cos t$:\n",
    "\n",
    "$$\n",
    "\\cos A = I - \\frac{A^2}{2!} + \\frac{A^4}{4!} - \\frac{A^6}{6!} + \\cdots .\n",
    "$$\n",
    "\n",
    "**(a)** If $Ax = \\lambda x$, multiply each term in the series by $x$ to find an eigenvalue of $\\cos A$.\n",
    "\n",
    "**(b)** Explain, using the series, why $\\frac{d^2}{dt^2} \\cos(At) = -A^2 \\cos(At)$.\n",
    "\n",
    "**(c)** Explain why $u(t) = \\cos(At) u(0)$ solves $\\frac{d^2 u}{dt^2} = -A^2 u$, given the initial conditions $u(0)$ and $\\left . \\frac{du}{dt} \\right|_{t=0} = 0$.\n",
    "\n",
    "**(d)** If $A = \\begin{pmatrix} \\pi & \\pi \\\\ \\pi & \\pi \\end{pmatrix}$, it has eigenvectors $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $v_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.   Find the corresponding eigenvalues, and use them to compute the matrix $\\cos A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)**  Following the instructions, we get $$(\\cos A)x = (I - \\frac{A^2}{2!} + \\frac{A^4}{4!} - \\frac{A^6}{6!} + \\cdots)x = x - \\frac{\\lambda^2}{2!}x + \\frac{\\lambda^4}{4!}x - \\frac{\\lambda^6}{6!}x + \\cdots = \\cos(\\lambda)x,$$, where we have used the ordinary 18.01 infinite series for $\\cos\\lambda$, so $x$ is again an eigenvector of $\\cos A$ with eigenvalue $\\cos(\\lambda)$.\n",
    "\n",
    "**(b)**  Taking the derivatives using the series, we get $$\\frac{d^2}{dt^2}\\cos(At) = \\frac{d^2}{dt^2}(I - \\frac{t^2A^2}{2!} + \\frac{t^4A^4}{4!} - \\frac{t^6A^6}{6!} + \\cdots) = 0 - \\frac{A^2}{1} + \\frac{t^2A^4}{2!} - \\frac{t^4A^6}{4!} + \\cdots = -A^2(I - \\frac{t^2A^2}{2!} + \\frac{t^4A^4}{4!} - \\frac{t^6A^6}{6!} + \\cdots) = -A^2\\cos(At)$$ as needed.  Notice that it also equals $-\\cos(At)A^2$ ($A$ commutes with itself, so any of these power series functions of $A$ will also commute with $A$.)\n",
    "\n",
    "**(c)**  First, notice $\\cos(0A) = I$ and it follows that the initial value $u(0)$ is satisfied: $$\\cos(A\\cdot 0)u(0) = Iu(0) = u(0).$$  Also, notice that $$\\frac{du}{dt} = \\frac{d}{dt}(\\cos(tA))u(0) = \\left(\\frac{d}{dt}\\cos(tA)\\right)u(0)$$ and similarly for the second derivative (using part (b)): $$\\frac{d^2u}{dt^2} = \\frac{d^2}{dt^2}(\\cos(tA))u(0) = \\left(\\frac{d^2}{dt^2}\\cos(tA)\\right)u(0) = -A^2\\cos(tA)u(0) = -A^2u(t).$$ This equation above shows that the differential equation $\\frac{d^2}{dt^2} u(t) = -A^2u(t)$ holds.  The remaining initial condition $\\left. \\frac{du}{dt}\\right|_{t = 0} = 0$ holds because the linear term (coefficient of $t$) in the power series expansion for $\\cos(tA)$ is 0.\n",
    "\n",
    "**(d)**  The first column of $A$ plus the second column is $(2\\pi, 2\\pi) = 2\\pi(1, 1)$, which means that $\\boxed{v_1 = (1, 1) \\text{ is an eigenvector with eigenvalue } 2\\pi.}$  Similarly, the first column of $A$ *minus* the second column of $A$ is 0, so $\\boxed{v_2 = (1, -1) \\text{ is an eigenvector with eigenvalue 0.}}$  So $\\cos(A)$ is a $2 \\times 2$ matrix with eigenvector $v_1 = (1, 1)$ with eigenvalue $\\cos(2\\pi) = 1$ and with eigenvector $v_2 = (1, -1)$ with eigenvalue $\\cos(0) = 1$ as well!  But as $v_1, v_2$ form a basis for the plane, every vector in the plane is an eigenvector for $\\cos(A)$ with eigenvalue 1, which means that $\\cos(A)x = x$ for all $x \\in \\mathbb{R}^2$.  So $\\boxed{\\cos(A) = I.}$  For a sanity check, we can make sure this agrees with the series definition.  Notice that $A^n = (2\\pi)^n(A/2\\pi)$ for all $n \\geq 1$.  So $$\\cos(A) = I - A^2/2! + A^4/4 - A^6/6! \\cdots = I + (-(2\\pi)^2/2! + (2\\pi)^4/4! - \\cdots)(A/2\\pi) = I + (\\cos(2\\pi) - 1)(A/2\\pi) = I + (1 - 1)(A/2\\pi) = I,$$ as we already saw in a more conceptual way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (10+10+10 points)\n",
    "\n",
    "Suppose that $A$ is a $3\\times 3$ real-symmetric matrix with eigenvalues $\\lambda_1 = 0$, $\\lambda_2 = -1$, and $\\lambda_3 = -2$.    You are given two corresponding eigenvectors $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$ (for $\\lambda_1$) and $v_3 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$ (for $\\lambda_3$).\n",
    "\n",
    "**(a)** Give an approximate solution at $t = 100$ to $\\frac{dx}{dt} = Ax$ for $x(0) = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.  (Give the *numerical* value of a specific vector — no unknown coefficients or symbolic expressions!)\n",
    "\n",
    "**(b)** Give an eigenvector $v_2$ for $\\lambda_2$ and write the matrix $A$ as a product of three matrices.  (You shouldn't need your answer here to answer part a!)\n",
    "\n",
    "**(c)** Suppose that we compute the sequence $x_0, x_1, x_2, \\ldots$ given by the recurrence $x_{n+1} = \\alpha A x_n$ for some scalar $\\alpha$.   For what value(s) of α would you expect $x_n$ to approach **oscillating** (not exponentially growing, decaying, or constant) solutions for large $n$, for most initial $x_0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)**  For any initial condition $x(0)$, we can write it as a linear combination $x(0) = c_1v_1 + c_2v_2 + c_3v_3$, where $v_1, v_3$ are as given and $v_2$ is an eigenvector of $A$ with eigenvalue $\\lambda_2 = -1$.  Then the solution $x(t)$ to the differential equation $\\frac{dx}{dt} = Ax$ with initial condition $x(0)$ is $$x(t) = c_1e^{\\lambda_1t}v_1 + c_2e^{\\lambda_2t}v_2 + c_3e^{\\lambda_3t}v_3 = c_1v_1 + c_2e^{-t}v_2 + c_3e^{-2t}v_3.$$  So, as $t \\rightarrow \\infty$, the last two terms go to zero and $x(t) \\rightarrow c_1v_1$.  The matrix $A$ is *symmetric*, so the eigenvectors $v_1, v_2, v_3$ (with *distinct* eigenvalues) are orthogonal, so $c_1v_1$ is just the *orthogonal* projection of $x(0)$ to the line spanned by $v_1$!  So, we have $$x(100) \\sim \\frac{v_1v_1^T}{v_1^Tv_1}x(0) = (1/2)(1)(v_1) = \\boxed{\\begin{pmatrix}1/2 \\\\ 0 \\\\ 1/2\\end{pmatrix}}.$$\n",
    "\n",
    "**(b)**  Because $A$ is symmetric with distinct eigenvalues, any eigenvector $v_2$ with eigenvalue $\\lambda_2$ must be in the orthogonal complement of $\\text{span}\\{v_1, v_3\\}$.  The vectors $v_1, v_3$ span a plane, so this orthogonal complement is a line, and therefore any nonzero vector on this line will do.  To be orthogonal to $v_3$ means that the second coordinate is zero, and to be orthogonal to $v_1$ means that the sum of the first and third coordinates is zero.  So $\\boxed{v_2 = (1, 0, -1)}$ will work.  We can then write $A$ as the following product of 3 matrices, as discussed in class: $$X \\Lambda X^{-1} = \\begin{pmatrix}1 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & -1 & 0\\end{pmatrix}\\begin{pmatrix} 0 &0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & -2\\end{pmatrix}\\begin{pmatrix}1 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & -1 & 0\\end{pmatrix}^{-1}.$$\n",
    "\n",
    "**(c)**  For any $\\alpha$, the eigenvalues of $\\alpha A$ are $0, -\\alpha, -2\\alpha$.  If $\\alpha = 0$ then $A = 0$ and we would get constant solutions, which we don't want, so we can assume that $\\alpha \\neq 0$.  In that case, the largest eigenvalue by absolute value is $-2\\alpha$, and the corresponding eigenspace is 1-dimensional.  So, as $n \\rightarrow \\infty$ we expect $x_n$ to be roughly, relative to its size, a scalar multiple of an eigenvector of $A$ with eigenvalue $-2\\alpha$ (precisely, this will happen as long as the initial $x_0$ has a nonzero component in this eigenspace).  For this solution to approach an oscillating solution, we need that $\\boxed{|2\\alpha| = 1.}$\n",
    "\n",
    "(Note that this asymptotic solution won't actually be *periodic* unless $(2\\alpha)^N = 1$ for some integer $N$, i.e. when $2\\alpha$ is a [root of unity](https://en.wikipedia.org/wiki/Root_of_unity).)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
