{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 13 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "In class, we considered the 1d scalar wave equation\n",
    "$$\n",
    "\\frac{\\partial^2 u}{\\partial t^2} = \\frac{\\partial^2 u}{\\partial x^2}\n",
    "$$\n",
    "for $u(x,t)$ defined on $x\\in [0,L]$ with $u(0,t) = u(L,t) = 0$, and found that the solutions were oscillating.  (We ignore the technicalities of nailing down the [precise function space](https://en.wikipedia.org/wiki/Sobolev_space) where our derivatives and integrals exist.)\n",
    "\n",
    "Another way to write this equation is to define a new variable $v(x,t)$ so that $\\frac{\\partial v}{\\partial x} = \\frac{\\partial u}{\\partial t}$, in which case the wave equation above implies $\\frac{\\partial v}{\\partial t} = \\frac{\\partial u}{\\partial x}$.  In terms of $u$ and $v$, the wave equation can now be written:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial t} \\underbrace{\\begin{pmatrix} u \\\\ v \\end{pmatrix}}_\\vec{w} = \\underbrace{\\begin{pmatrix} 0 & \\frac{\\partial}{\\partial x} \\\\ \\frac{\\partial}{\\partial x} & 0 \\end{pmatrix}}_A \\underbrace{\\begin{pmatrix} u \\\\ v \\end{pmatrix}}_\\vec{w}\n",
    "$$\n",
    "where we have defined a two-component vector of *functions* $\\vec{w}(x,t) = (u\\;v)^T$ and a 2x2 \"matrix\" (linear operator) $A$ of derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Define the inner product $\\vec{w}_1 \\cdot \\vec{w}_2 = \\int_0^L \\vec{w}_1(x)^H \\vec{w}_2(x) dx$ for any vector-valued functions $\\vec{w}_k(x) = \\begin{pmatrix} u_k(x) \\\\ v_k(x) \\end{pmatrix}$.   Show via integration by parts, assuming $u_k(0)=u_k(L)=0$, that\n",
    "$$\n",
    "\\vec{w}_1 \\cdot (A\\vec{w}_2) = (-A\\vec{w}_1) \\cdot \\vec{w}_2\n",
    "$$\n",
    "for the $A$ above.  That is, the linear operator $A$ is **anti-symmetric** (anti-Hermitian, also called \"skew\" Hermitian): \"$A^H = -A$\".\n",
    "\n",
    "Review problem 3 of pset 11: there, you found that if $\\frac{dx}{dt} = Ax$ where $A^H = -A$, then $\\Vert x(t) \\Vert$ was a constant (\"conserved\") over time, and the eigenvalues were purely imaginary (oscillating solutions).   Now, show the same things here:\n",
    "\n",
    "**(b)** If $\\frac{\\partial \\vec{w}}{\\partial t} = A\\vec{w}$ (i.e. $\\vec{w}$ solves the wave equation), show (using the property from (a)) that:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial t} \\Vert \\vec{w} \\Vert^2 = \\frac{\\partial}{\\partial t} (\\vec{w} \\cdot \\vec{w}) = \\frac{\\partial\\vec{w}}{\\partial t} \\cdot \\vec{w} + \\vec{w} \\cdot \\frac{\\partial\\vec{w}}{\\partial t} = \\cdots = 0.\n",
    "$$\n",
    "That is, show that $\\Vert \\vec{w} \\Vert^2 = \\int_0^L (|u|^2 + |v|^2)dx$ is a *constant* in time.  In most physical wave equations, this represents *conservation of energy*.\n",
    "\n",
    "**(c)** If $A \\vec{w} = \\lambda \\vec{w}$ for $\\vec{w} \\ne 0$ (i.e. an eigenfunction), show using part (a) that $\\lambda$ is *purely imaginary*.  (Hint: take the dot product of both sides with $\\vec{w}$ and apply the result from (a).)  This shows (again) that we have *oscillating* solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** We can show that $A$ is **anti-symmetric** as follows.  Take any functions $\\vec{w}_1(x)$ and $\\vec{w}_2(x)$ satisfying our boundary conditions on $u_k$.   (Note that $A$ has no time derivatives, so it is irrelevant whether the functions are time-dependent.)   Let $u'(x)$ and $v'(x)$ denote the derivatives with respect to $x$ as usual. Plugging these in, we find:\n",
    "\\begin{align}\n",
    "\\vec{w}_1 \\cdot(A\\vec{w}_2) &= \\int_0^L \\vec{w}_1^H(x)A\\vec{w}_2(x) dx \\\\\n",
    "&= \\int_0^L \\begin{pmatrix} \\overline{u_1(x)} & \\overline{v_1(x)} \\end{pmatrix} \\begin{pmatrix} 0 & \\frac{\\partial}{\\partial x} \\\\ \\frac{\\partial}{\\partial x} & 0 \\end{pmatrix}\\begin{pmatrix} u_2(x) \\\\ v_2(x) \\end{pmatrix} dx\\\\\n",
    "&= \\int_0^L \\begin{pmatrix} \\overline{u_1(x)} & \\overline{v_1(x)} \\end{pmatrix} \\begin{pmatrix} v_2'(x) \\\\ u_2'(x) \\end{pmatrix} dx \\\\\n",
    "&= \\int_0^L \\left[ \\overline{u_1(x)} v_2'(x) + \\overline{v_1(x)} u_2'(x) \\right] dx\\\\\n",
    "&= \\underbrace{\\left[\\overline{u_1} v_2 + \\overline{v_1} u_2\\right]_0^L}_{=0\\mbox{ since }u(0)=u(L)=0} - \\int_0^L \\left[ \\overline{u_1'(x)} v_2(x) + \\overline{v_1'(x)} u_2(x) \\right] dx \\\\\n",
    "&= -\\int_0^L \\begin{pmatrix} \\overline{v_1'(x)} & \\overline{u_1'(x)} \\end{pmatrix} \\begin{pmatrix} u_2(x)\\\\  v_2(x) \\end{pmatrix} dx \n",
    "\\end{align}\n",
    "Where we have used the fact that $u_k(0)=u_k(L)=0$. Then we have that:\n",
    "\\begin{align}\n",
    "\\vec{w}_1 \\cdot(A\\vec{w}_2)&= \\int_0^L \\left[-\\begin{pmatrix} 0 & \\frac{\\partial}{\\partial x} \\\\ \\frac{\\partial}{\\partial x} & 0 \\end{pmatrix}\\begin{pmatrix} u_1(x) \\\\ v_1(x) \\end{pmatrix}\\right]^H \\begin{pmatrix} u_2(x) \\\\ v_2(x) \\end{pmatrix}dx\\\\\n",
    "&=(-A\\vec{w}_1)\\cdot\\vec{w}_2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** We can show that $\\Vert \\vec{w} \\Vert^2$ is conserved as follows:\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial t} \\Vert \\vec{w} \\Vert^2 = \\frac{\\partial}{\\partial t} (\\vec{w} \\cdot \\vec{w}) \n",
    "&= \\frac{\\partial\\vec{w}}{\\partial t} \\cdot \\vec{w} + \\vec{w} \\cdot \\frac{\\partial\\vec{w}}{\\partial t} \\\\\n",
    "&= (A\\vec{w})\\cdot \\vec{w} + \\vec{w}\\cdot (A\\vec{w})\\\\\n",
    "&= (A\\vec{w})\\cdot \\vec{w} + (-A\\vec{w})\\cdot \\vec{w}\\\\\n",
    "&= (A\\vec{w})\\cdot \\vec{w} - (A\\vec{w})\\cdot \\vec{w} = 0.\n",
    "\\end{align}\n",
    "Therefore $\\Vert w \\Vert^2$ is constant in time.\n",
    "\n",
    "**(c)** To conclude that $A$ has purely imaginary eigenvalues, we take the inner product of the eigenvalue equation $A\\vec{w} = \\lambda \\vec{w}$ with an eigenvector $\\vec{w}\\ne 0$:\n",
    "\\begin{align}\n",
    "&\\vec{w}\\cdot(A\\vec{w})=\\vec{w}\\cdot(\\lambda\\vec{w})\\\\\n",
    "&\\implies \\vec{w}\\cdot(A\\vec{w})=\\lambda\\Vert\\vec{w}\\Vert^2\\\\\n",
    "&\\implies - (A\\vec{w})\\cdot\\vec{w} = \\lambda \\Vert\\vec{w}\\Vert^2\\\\\n",
    "&\\implies - \\bar{\\lambda}\\Vert\\vec{w}\\Vert^2 = \\lambda \\Vert\\vec{w}\\Vert^2\\\\\n",
    "&\\implies (\\lambda+\\bar{\\lambda}) \\Vert\\vec{w}\\Vert^2 = 0\\\\\n",
    "&\\implies \\lambda + \\bar{\\lambda} = 0\\\\\n",
    "&\\implies \\boxed{\\text{Re}(\\lambda) = 0.}\n",
    "&\\end{align}\n",
    "Hence $\\lambda$ is purely imaginary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Review problem: suppose that $x_1, x_2, \\cdots x_n$ are a set of $n$ orthogonal (but not orthonormal) vectors in $\\mathbb{R}^n$, forming the columns of a matrix $X$. \n",
    "\n",
    "**(a)** If we are computing the orthogonal projection $\\frac{x_1 x_1^T}{x_1^T x_1} x$, we can do the *same* calculation in two ways just by putting parentheses in different places.  One way has a number of arithmetic operations proportional to $n$, and another way has a number of arithmetic operations proportional to $n^2$. What are the two ways?  Which would you choose?\n",
    "\n",
    "**(b)** If we write a vector $x$ in this basis, we have:\n",
    "$$\n",
    "x = c_1 x_1 + c_2 x_2 + \\cdots + c_n x_n = Xc\n",
    "$$\n",
    "What is the most efficient way you know to compute $c_1$?   How does the number of arithmetic operations scale with $n$?   How does this compare to solving $Xc=x$ for $c$ by Gaussian elimination?  Which would you choose?\n",
    "\n",
    "**(c)** Explain why you can write $X^{-1}$ as the product of $X^T$ and a *diagonal* matrix.  What are the entries of the diagonal matrix, and does it multiply $X^T$ on the left or right?\n",
    "\n",
    "**(d)** Check your answer from (c) in Julia for the following 4 orthogonal vectors.  Note that you can form a diagonal matrix with diagonal entries $a,b,c,d$ by `Diagonal([a,b,c,d])` in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "**(a)** There are two ways to calculate the orthogonal projection $\\frac{x_1 x_1^T}{x_1^T x_1} x$\n",
    "* We can compute $x_1^Tx$ and $x_1^T x_1$, compute their ratio $\\frac{x_1Tx}{x_1^Tx_1}$ and then multiply by the vector $x_1$. The most computationally expensive part of this approach is computing the dot products $x_1^Tx$ and $x_1^T x_1$. Computing each of these requires $n$ scalar multiplications and additions. This approach to calculating the orthogonal projection will therefore require a number of arithmetic operations proprotional to $n$.\n",
    "* We can also explicitly calculate the $n\\times n$ matrix $x_1x_1^T$, apply this matrix to $x$, and then divide by $x_1^Tx_1$. The most computationally expensive part of this approach is calculating the matrix $x_1x_1^T$. This matrix has $n^2$ entries, each of which is a product of two elements of $x_1$. Thus this approach will have a number of arithmetic operations proportional to $n^2$. \n",
    "\n",
    "It is clear that in general the first method is the most sensible choice, since this is faster by a factor of $n$.\n",
    "\n",
    "**(b)** Since the $x_i$ are an orthogonal set, we can compute $c_1$ by taking the inner product of $x$ expressed in the eigenvector basis with $x_1$. Since $x_1^Tx_i = 0$ except for $i=1$, we then have that\n",
    "$$ x_1^Tx = c_1 x_1^Tx_1 \\implies \\boxed{c_1 = \\frac{x_1^Tx}{x_1^Tx_1}}.$$\n",
    "The number of arithmetic operations required to compute $c_1$ via this method requires a number of arithmetic operations that scales with $n$. Computing all of the $c_i$ using this approach would then require a number of arithmetic operations that scales with $n^2$. This is much faster than computing $c$ by solving $Xc=x$ using Gaussian elimination, which requires $n^3$ arithmetic operations. It is therefore **always better to use dot products** when you want to write a vector in terms of a basis of **orthogonal** vectors!\n",
    "\n",
    "**(c)** If the columns of $X$ were orthonormal, then the inverse of $X$ would be its transpose. Since the vectors $x_1$ are not orthogonal, but not normalized, we can compute $X^{-1}$ by taking the transpose of $X$ and then dividing each row by the norm squared of each row.   More explicitly, since $X^T X = D$ where $D$ is the diagonal matrix of the length² of each vector, we have $(D^{-1} X^T) X = I$, so:\n",
    "$$X^{-1} = D^{-1}X^T,$$\n",
    "where \n",
    "$$D^{-1} = \\begin{pmatrix} (x_1^Tx_1)^{-1} & & & \\\\ & (x_2^Tx_2)^{-1} & & \\\\ & & \\ddots & \\\\ & & & (x_n^Tx_n)^{-1} \\end{pmatrix}$$\n",
    "\n",
    "**(d)** We can check this result explicitly in Julia for some set of orthogonal vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Int64,2}:\n",
       " 1   2   1   1\n",
       " 1  -2   1  -1\n",
       " 1   1  -1  -2\n",
       " 1  -1  -1   2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = [1,1,1,1]\n",
    "x2 = [2,-2,1,-1]\n",
    "x3 = [1,1,-1,-1]\n",
    "x4 = [1,-1,-2,2]\n",
    "X = [x1 x2 x3 x4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Int64,2}:\n",
       " 4   0  0   0\n",
       " 0  10  0   0\n",
       " 0   0  4   0\n",
       " 0   0  0  10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X'X # check orthogonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       " 0.25   0.25   0.25   0.25\n",
       " 0.2   -0.2    0.1   -0.1 \n",
       " 0.25   0.25  -0.25  -0.25\n",
       " 0.1   -0.1   -0.2    0.2 "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       " 0.25   0.25   0.25   0.25\n",
       " 0.2   -0.2    0.1   -0.1 \n",
       " 0.25   0.25  -0.25  -0.25\n",
       " 0.1   -0.1   -0.2    0.2 "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X' multiplied on the left by the diagonal matrix D given in part (c) above\n",
    "# should match inv(X) above.\n",
    "\n",
    "D⁻¹ = Diagonal([1/(x1'x1), 1/(x2'x2), 1/(x3'x3), 1/(x4'x4)])\n",
    "D⁻¹*X'        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus see that $X^{-1}$ and $D^{-1} X^T$ give the same result. \n",
    "\n",
    "Alternatively, we could have gotten $D$ from the diagonal entries of the matrix $A = X^T X$ above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       " 0.25   0.25   0.25   0.25\n",
       " 0.2   -0.2    0.1   -0.1 \n",
       " 0.25   0.25  -0.25  -0.25\n",
       " 0.1   -0.1   -0.2    0.2 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Diagonal(A) \\ X'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
