{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 10 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (10+10+10+10 points)\n",
    "\n",
    "If $M$ is an $m\\times m$ Markov matrix with positive entries, consider the system of ODEs $$\\frac{dx}{dt} = (M-I)x$$.\n",
    "\n",
    "**(a)** Explain why the sum of the components of $x(t)$ is \"conserved\": that is, $o^T x(t)$ is a constant in time, where $o$ is the vector of $m$ 1's.  (Hint: look at problem 2 of pset 5.)\n",
    "\n",
    "**(b)** What do the eigenvalues of $M$ (positive Markov: one λ=1 eigenvalue and the remainder have $|\\lambda|<1$) tell you about the solution $x(t)$ for large $t$?  Does it blow up, oscillate, decay to zero, or … ?\n",
    "\n",
    "**(c)** Given the initial condition $x(0)$, give a formula for the solution $x(\\infty) = \\lim_{t\\to\\infty} x(t)$ in terms of $x(0)$, $o$, and the \"steady-state\" eigenvector $v_0$ ($M v_0 = v_0$) of $M$.\n",
    "\n",
    "**(d)** Check your answers to (a) and (c) by using the following Julia code to generate a random 5×5 positive Markov matrix, a random initial condition, and evaluating $x(t)=e^{(M-I)t} x(0)$ via `expm((M-I)*t)*x₀` for one or more values of `t` in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)**  A function $f(t)$ is constant if and only if $\\frac{df}{dt} = 0.$  Computing the derivative of the quantity $o^Tx(t)$ of interest, we get $$\\frac{d(o^Tx)}{dt} = o^T\\frac{dx}{dt} = o^T(M – I)x(t)$$ where in the first equality we used the fact that multiplication by a constant matrix commutes with differentiation and in the second equality we used the differential equation that $x(t)$ is said to satisfy.  But for any Markov matrix $N$ we have $o^TN = o$, and in particular this is true for the Markov matrices $M$ and $I$.  So $o^T(M – I) = o^T – o^T = 0$, and from the calculation above we then see that $\\boxed{\\frac{d(o^Tx)}{dt} = 0}$, so $o^Tx(t)$ is constant.\n",
    "\n",
    "This is very similar to problem 2 of pset 5: in the language of that problem, since $o$ is in the *left nullspace* of $M-I$, it follows that $o^T x$ is conserved by this ODE.  Equivalently, since $o$ is an eigenvector of $M^T$ with eigenvalue 1, it is in the nullspace of $M^T - I$, or the left nullspace of $M - I$.\n",
    "\n",
    "**(b)**  Recall that if $\\lambda_1, …, \\lambda_n$ are the eigenvalues of a matrix $A$ and if $c$ is a constant, then $\\lambda_1 – c, …, \\lambda_n – c$ are the eigenvalues of $A – cI$.  In particular, the eigenvalues of $M – I$ are just the eigenvalues of $M$ shifted by -1.  But the eigenvalues of $A$ are all either 1 (exactly one of those) or have real part less than 1.  So the eigenvalues of $A – I$ are all either 0 (exactly one of those) or have strictly negative real part.  For any complex number $z$ with strictly negative real part, we have $\\lim_{t \\rightarrow \\infty} e^{zt} = 0$.  So, writing $x(0) = c_1v_1 + \\cdots c_mv_m$ in a basis of eigenvectors for $M - I$ with $v_0$ of having eigenvalue 0 and all other $v_i$ for $i > 0$ having eigenvalue $\\lambda_i$ with strictly negative real part, the solution $$x(t) =c_1e^{0t}v_0 + c_1e^{\\lambda_1t}v_m + \\cdots = c_0v_0 + c_1e^{\\lambda_1t}v_1 + \\cdots $$ will tend towards $c_0v_0$, i.e. tend towards a constant vector.\n",
    "\n",
    "*Technical point*: Not all Markov matrices $M$ admit bases of eigenvectors, i.e. not all Markov matrices are diagonalizable.  So to make this argument fully general we’d need to use techniques for non-diagonalizable matrices that we'll see later.\n",
    "\n",
    "**(c)** From parts (a) and (b) above, we see that $x(\\infty)$ should be a multiple of the given steady state vector $v_0$ (of course, any multiple of $v_0$ is also a steady-state eigenvector), and also $o^Tx(\\infty) = o^Tx(0)$.  So, for some constant $c$, we have $x(\\infty) = cv _0$ and $o^Tx(\\infty) = o^Tx(0)$.  Hence $o^Tx(0) = o^Tx(\\infty) = o^Tcv_0 = co^Tv_0$ and therefore $c = o^Tx(0)/o^Tv_0$ so $$\\boxed{x(\\infty) = \\frac{o^Tx(0)}{o^Tv_0}v_0.}$$  As a sanity check to make sure that this is correct, notice that it depends linearly on $x(0)$, as it should, and that it does the right thing for steady-state $v_0$, as it should.\n",
    "\n",
    "**(d)**  Running the code below, we see that it agrees with the reasoning above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " 0.339949   0.0793109  0.189572  0.121393  0.0929254\n",
       " 0.350198   0.297788   0.077559  0.181762  0.0198272\n",
       " 0.228432   0.183546   0.187884  0.265823  0.600058 \n",
       " 0.0288393  0.200573   0.273508  0.299939  0.166957 \n",
       " 0.0525816  0.238783   0.271477  0.131083  0.120233 "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a random 5×5 Markov matrix\n",
    "M = rand(5,5)\n",
    "M = M ./ sum(M, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×1 Array{Float64,2}:\n",
       " 0.359045\n",
       " 0.375283\n",
       " 0.615814\n",
       " 0.453759\n",
       " 0.380953"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v₀ = nullspace(M-I) # the λ=1 eigenvector of M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×1 Array{Float64,2}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = ones(v₀) # the vector of 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       "  1.00555 \n",
       "  0.237402\n",
       "  0.496208\n",
       " -0.811531\n",
       "  0.490014"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x₀ = randn(length(o)) # a random initial condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 0.232966\n",
       " 0.243502\n",
       " 0.39957 \n",
       " 0.294421\n",
       " 0.247181"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 100\n",
    "x = expm((M-I) * t) * x₀  # the solution x(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4176404788401358, 1.4176404788401318)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x), sum(x₀)   # compare the sums of the components of x(t) and x(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×1 Array{Float64,2}:\n",
       " -6.66134e-16\n",
       " -7.21645e-16\n",
       " -1.27676e-15\n",
       " -7.77156e-16\n",
       " -6.10623e-16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(x₀)/(sum(v₀)))*v₀ - x # enter your formula from (c) and compare it to x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is $\\approx 0$ (up to roundoff errors), so our answer from (c) looks right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (5+5+10+10+5 points)\n",
    "\n",
    "Suppose that we have a second-order system of ODEs, of the form:\n",
    "$$\n",
    "\\ddot{x} = \\frac{d^2 x}{dt^2} = A x\n",
    "$$\n",
    "\n",
    "where $A$ is an $m \\times m$ matrix and we use $\\dot{x}$ to denote $dx/dt$ and $\\ddot{x}$ to denote $d^2 x/dt^2$.\n",
    "\n",
    "If $A=a$ and $x$ are scalars, this becomes the scalar ODE $\\frac{d^2 x}{dt^2} = a x$.  If we let $\\kappa = \\sqrt{a}$, then it is easy to check that the solutions are of the form $x(t) = c e^{+\\kappa t} + d e^{-\\kappa t}$ where $c$ and $d$ are determined by initial conditions $x(0)$ and $\\dot{x}(0)$.\n",
    "\n",
    "**(a)** For the scalar case, show that the same solution can also be written $x(t) = \\alpha \\cosh(\\kappa t) + \\beta \\sinh(\\kappa t)$ in terms of the [hyperbolic functions](https://en.wikipedia.org/wiki/Hyperbolic_function) cosh and sinh.  What are $\\alpha$ and $\\beta$ in terms of $c$ and $d$ from the solution above?  What are $\\alpha$ and $\\beta$ in terms of $x(0)$ and $\\dot{x}(0)$?\n",
    "\n",
    "**(b)** For the scalar case, if $a < 0$ then we have a purely imaginary $\\sqrt{a} = \\kappa = i\\omega$ for a real $\\omega$.   Using your answers from the previous part, write $x(t)$ as a purely real function (for real initial conditions) in terms of $\\cos(\\omega t)$ and $\\sin(\\omega t)$.   Note that $\\cosh(iy) = \\cos(y)$ and $\\sinh(iy) = i\\sin(y)$.\n",
    "\n",
    "**(c)** If $v$ is an eigenvector of $A$ with eigenvalue $\\lambda$, find a solution $x(t)$ of $\\ddot{x} = A x$ given by $v$ multiplied by some scalar function $f(t)$.   (Hint: for $v$, remember that $A$ acts like a scalar.  Your function can be written in terms of $\\cosh$ and $\\sinh$ of ...?)\n",
    "\n",
    "**(d)** Suppose $A$ is diagonalizable, with a basis eigenvectors $v_1, \\ldots, v_m$ and eigenvalues $\\lambda_1, \\ldots, \\lambda_m$.   Expand your solution $x(t)$ in the basis of these eigenvectors by adding up your answers from (c).   If we write the initial conditions in the basis of the eigenvectors, $x(0) = \\sum_k c_k v_k$ and $\\dot{x}(0) = \\sum_k d_k v_k$ for coefficients c and d, you should be able to give an **explicit formula for x(t)** in terms of the $c_k$, $d_k$, $v_k$, and $\\lambda_k$.\n",
    "\n",
    "**(e)** You will get *sinusoidally oscillating* (not growing or decaying) solutions if all of the eigenvalues of A are .........?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)**  By definition we have $\\sinh(x) = (e^x - e^{-x})/2$ and $\\cosh(x) = (e^x + e^{-x})/2$.  It follows that $e^{-x} = \\cosh(x) - \\sinh(x)$ and $e^x = \\cosh(x) + \\sinh(x)$.  Therefore $$x(t) = ce^{\\kappa t} + de^{-\\kappa t} = c(\\cosh(\\kappa t) + \\sinh(\\kappa t)) + d(\\cosh(\\kappa t) - \\sinh(\\kappa t)) = (c + d)\\cosh(\\kappa t) + (c - d)\\sinh(\\kappa t).$$  So taking $\\boxed{\\alpha = c + d}$ and $\\boxed{\\beta = c - d}$ does the trick.\n",
    "\n",
    "In terms of the initial conditions $x(0)$ and $\\dot{x}(0)$, we have $\\boxed{x(0) = \\alpha}$ and, from $\\dot{x}(t) = - \\kappa \\alpha \\sinh(\\kappa t) + \\kappa \\beta \\cosh(\\kappa t)$ we get $\\boxed{\\beta = \\dot{x}(0) / \\kappa}$.\n",
    "\n",
    "**(b)**  Just plugging in $\\kappa = i\\omega$ into part (a), we get $$x(t) = \\alpha \\cosh(i\\omega t) + \\beta \\sinh(i\\omega t) = \\boxed{\\alpha \\cos(\\omega t) + i \\beta \\sin(\\omega t)}.$$  Note that this *is* a real valued function.   To check that this is real-valued, we should plug our α and β from above in terms of the initial conditions: $\\alpha = x(0)$ and $\\beta = \\dot{x}(0) / i \\omega$.  Plugging this into $x(t)$, we get: $$\\boxed{x(t) = x(0) \\cos(\\omega t) + \\frac{\\dot{x}(0)}{\\omega} \\sin(\\omega t)}$$\n",
    "which is obviously purely real (all the i's have cancelled) for real initial conditions and real $\\omega \\ne 0$.\n",
    "\n",
    "**(c)**  Let's suppose we can do as we're asked, so there is some solution of the form $x(t) = f(t)v$.  Then what is the differential equation asking of $f(t)$?  The second derivative is $\\frac{d^2x}{dt^2} = \\ddot{f}(t)v$, and as $Av = \\lambda v$ so $Ax(t) = Af(t)v = f(t)\\lambda v$ we see that $x(t) = f(t)v$ is a solution if and only if $\\ddot{f}(t) = \\lambda f(t)$, so we've reduced to the scalar case (thanks, eigenvector!).  From part (a) we know the full set of solutions to this equation in terms of parameters $\\alpha, \\beta$, so the solutions are precisely the functions of the form $$\\boxed{x(t) = (\\alpha\\cosh(\\sqrt{\\lambda} t) + \\beta\\sinh(\\sqrt{\\lambda}t))v}$$.\n",
    "\n",
    "**(d)**  Since the differential equation we're concerned with is linear (this is not a coincidence with the fact that you are taking a linear algebra class), its solution set is *linear*, i.e. any linear combination of solutions is a solution.  So, we can first deal with initial conditions of the form $x(0) = c_kv_k$ and $\\dot{x}(0) = d_kv_k$ for some fixed index $k$, and then just add them up - the initial conditions depened linearly on the solutions, so the initial conditions of the sum will be what we want.  For the initial conditions $x(0) = c_kv_k$ and $x'(0) = d_kv_k$, it follows from parts (a) and (c) that the solution is $$x(t) = \\left[c_k\\cosh(\\sqrt{\\lambda_k}t) + \\frac{d_k}{\\sqrt{\\lambda_k}} \\sinh(\\sqrt{\\lambda_k}t) \\right]v_k.$$  Summing over $k$ then gives the answer: $$\\boxed{x(t) = \\sum_{k = 1}^m \\left[c_k\\cosh(\\sqrt{\\lambda_k}t) + \\frac{d_k}{\\sqrt{\\lambda_k}} \\sinh(\\sqrt{\\lambda_k}t) \\right]v_k.}$$\n",
    "\n",
    "**(e)**  The functions $\\cosh(\\sqrt{\\lambda} t)$ and $\\sinh(\\sqrt{\\lambda} t)$ are sinusoidally oscillating if and only if $\\sqrt{\\lambda}$ has zero real part, i.e. when $\\lambda$ is **real and negative**.  So the solution will be sinusoidally oscillating if and only if $\\boxed{\\lambda_k \\mbox{ is real and } \\lambda_k < 0 \\text{ for all } k}$.\n",
    "\n",
    "(Note that the case $\\lambda = 0$ is special - the solutions to $\\ddot{f}(t) = \\lambda f(t)$ are the *linear functions* $f(t) = \\gamma + \\delta t$ in that case, so it may seem at first that we need to re-do everything.  Actually, however, if we take the limit $\\lambda\\to 0$ of $\\frac{1}{\\sqrt{\\lambda}} \\sinh(\\sqrt{\\lambda}t) \\to t$ we get precisely this second solution $t$!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (10+10+10 points)\n",
    "\n",
    "(From Strang, section 6.3, problem 8.)\n",
    "\n",
    "The following ODEs describe the population of rabbits (r) and wolves (w):\n",
    "$$\n",
    "\\frac{dr}{dt} = 6r - 2w, \\qquad \\frac{dw}{dt} = 2r + w .\n",
    "$$\n",
    "These equations make the rabbit population grow quickly (from $6r$) but decreases when there are more wolves (from $-2w$).  The wolf population always grows in this model.\n",
    "\n",
    "**(a)** Write this problem in matrix form, and find the eigenvalues and eigenvectors.\n",
    "\n",
    "**(b)** If $r(0)=w(0)=30$, give a formula for $r(t)$ and $w(t)$.\n",
    "\n",
    "**(c)** After a long time, what is the ratio of rabbits to wolves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "\n",
    "**(a)**  We have $$\\frac{d}{dt} \\begin{pmatrix} r \\\\ w\\end{pmatrix} = \\begin{pmatrix} 6r - 2w \\\\ 2r + 2\\end{pmatrix} = \\begin{pmatrix} 6 & -2 \\\\ 2 & 1\\end{pmatrix}\\begin{pmatrix} r \\\\ w\\end{pmatrix}$$ so the matrix form is $$\\boxed{\\frac{d}{dt} \\begin{pmatrix} r \\\\ w\\end{pmatrix} = \\begin{pmatrix} 6 & -2 \\\\ 2 & 1\\end{pmatrix}\\begin{pmatrix} r \\\\ w\\end{pmatrix}.}$$  The characteristic polynomial is $$\\det\\begin{pmatrix} 6 - \\lambda & -2 \\\\ 2 & 1 - \\lambda \\end{pmatrix} = \\lambda^2  - 7\\lambda + 10 = (\\lambda - 5)(\\lambda - 2)$$ so $\\boxed{\\text{the eigenvalues are } \\lambda = 2,5.}$  Taking $\\lambda = 2$, we need to find a vector in the nullspace of $$\\begin{pmatrix} 4 & -2 \\\\ 2 & -1\\end{pmatrix}.$$  One of the first column plus two of the second is zero, so we have an $\\boxed{\\text{eigenvector } (1, 2) \\text{ with eigenvalue } 2.}$  Taking $\\lambda = 5$, we need to find a vector in the nullspace of $$\\begin{pmatrix} 1 & -2 \\\\ 2 & -4\\end{pmatrix}.$$  The second column plus two of the first is zero, so we have an $\\boxed{\\text{eigenvector } (2, 1) \\text{ with eigenvalue } 5.}$  Up to nonzero scalar multiple, this describes all of the eigenvectors.\n",
    "\n",
    "**(b)**  The game is always to do the computation in a basis of eigenvectors.  We can write the initial conditions in the basis of eigenvectors from part (a):  $(30, 30) = 10v_2 + 10v_5$ where $v_2 = (1, 2)$ is an eigenvector of eigenvalue 2 and $v_5 = (2, 1)$ is an eigenvector of eigenvalue $5$.  This lets us write down the answer for the solution: $$\\boxed{\\begin{pmatrix} r(t) \\\\ w(t) \\end{pmatrix} = 10e^{2t}v_2 + 10e^{5t}v_5 = \\begin{pmatrix} 10e^{2t} + 20e^{5t} \\\\ 20e^{2t} + 10e^{5t}\\end{pmatrix}.}$$  You can do the sanity check that this satisfies the original differential equation and initial conditions.\n",
    "\n",
    "**(c)**  The ratio at time $t$ is $$\\frac{10e^{2t} + 20e^{5t}}{20e^{2t} + 10e^{5t}}$$ which has limit $\\boxed{2}$ as $t \\rightarrow \\infty$ (the $e^{5t}$ terms dominate).\n",
    "\n",
    "The key point here is that for $t \\rightarrow \\infty$, the eigenvector for the largest |λ| dominates, here $v_5$, so the result must goes to a multiple of $v_5$.   Since $v_5 = (2, 1)$, that means the ratio $r/w \\to 2/1 = 2$. \n",
    "\n",
    "Even if we had 100 eigenvectors (for 100 species of animal), and the solution was much more complicated, we'd still know the ratio of species for large $t$ just by looking at *one* eigenvector, the eigenvector for the largest |λ|.  (Unless multiple eigenvalues have the same magnitude.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
