{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 13\n",
    "\n",
    "Due Wednesday, December 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "In class, we considered the 1d scalar wave equation\n",
    "$$\n",
    "\\frac{\\partial^2 u}{\\partial t^2} = \\frac{\\partial^2 u}{\\partial x^2}\n",
    "$$\n",
    "for $u(x,t)$ defined on $x\\in [0,L]$ with $u(0,t) = u(L,t) = 0$, and found that the solutions were oscillating.  (We ignore the technicalities of nailing down the [precise function space](https://en.wikipedia.org/wiki/Sobolev_space) where our derivatives and integrals exist.)\n",
    "\n",
    "Another way to write this equation is to define a new variable $v(x,t)$ so that $\\frac{\\partial v}{\\partial x} = \\frac{\\partial u}{\\partial t}$, in which case the wave equation above implies $\\frac{\\partial v}{\\partial t} = \\frac{\\partial u}{\\partial x}$.  In terms of $u$ and $v$, the wave equation can now be written:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial t} \\underbrace{\\begin{pmatrix} u \\\\ v \\end{pmatrix}}_\\vec{w} = \\underbrace{\\begin{pmatrix} 0 & \\frac{\\partial}{\\partial x} \\\\ \\frac{\\partial}{\\partial x} & 0 \\end{pmatrix}}_A \\underbrace{\\begin{pmatrix} u \\\\ v \\end{pmatrix}}_\\vec{w}\n",
    "$$\n",
    "where we have defined a two-component vector of *functions* $\\vec{w}(x,t) = (u\\;v)^T$ and a 2x2 \"matrix\" (linear operator) $A$ of derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Define the inner product $\\vec{w}_1 \\cdot \\vec{w}_2 = \\int_0^L \\vec{w}_1(x)^H \\vec{w}_2(x) dx$ for any vector-valued functions $\\vec{w}_k(x) = \\begin{pmatrix} u_k(x) \\\\ v_k(x) \\end{pmatrix}$.   Show via integration by parts, assuming $u_k(0)=u_k(L)=0$, that\n",
    "$$\n",
    "\\vec{w}_1 \\cdot (A\\vec{w}_2) = (-A\\vec{w}_1) \\cdot \\vec{w}_2\n",
    "$$\n",
    "for the $A$ above.  That is, the linear operator $A$ is **anti-symmetric** (anti-Hermitian, also called \"skew\" Hermitian): \"$A^H = -A$\".\n",
    "\n",
    "Review problem 3 of pset 11: there, you found that if $\\frac{dx}{dt} = Ax$ where $A^H = -A$, then $\\Vert x(t) \\Vert$ was a constant (\"conserved\") over time, and the eigenvalues were purely imaginary (oscillating solutions).   Now, show the same things here:\n",
    "\n",
    "**(b)** If $\\frac{\\partial \\vec{w}}{\\partial t} = A\\vec{w}$ (i.e. $\\vec{w}$ solves the wave equation), show (using the property from (a)) that:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial t} \\Vert \\vec{w} \\Vert^2 = \\frac{\\partial}{\\partial t} (\\vec{w} \\cdot \\vec{w}) = \\frac{\\partial\\vec{w}}{\\partial t} \\cdot \\vec{w} + \\vec{w} \\cdot \\frac{\\partial\\vec{w}}{\\partial t} = \\cdots = 0.\n",
    "$$\n",
    "That is, show that $\\Vert \\vec{w} \\Vert^2 = \\int_0^L (|u|^2 + |v|^2)dx$ is a *constant* in time.  In most physical wave equations, this represents *conservation of energy*.\n",
    "\n",
    "**(c)** If $A \\vec{w} = \\lambda \\vec{w}$ for $\\vec{w} \\ne 0$ (i.e. an eigenfunction), show using part (a) that $\\lambda$ is *purely imaginary*.  (Hint: take the dot product of both sides with $\\vec{w}$ and apply the result from (a).)  This shows (again) that we have *oscillating* solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Review problem: suppose that $x_1, x_2, \\cdots x_n$ are a set of $n$ orthogonal (but not orthonormal) vectors in $\\mathbb{R}^n$, forming the columns of a matrix $X$. \n",
    "\n",
    "**(a)** If we are computing the orthogonal projection $\\frac{x_1 x_1^T}{x_1^T x_1} x$, we can do the *same* calculation in two ways just by putting parentheses in different places.  One way has a number of arithmetic operations proportional to $n$, and another way has a number of arithmetic operations proportional to $n^2$. What are the two ways?  Which would you choose?\n",
    "\n",
    "**(b)** If we write a vector $x$ in this basis, we have:\n",
    "$$\n",
    "x = c_1 x_1 + c_2 x_2 + \\cdots + c_n x_n = Xc\n",
    "$$\n",
    "What is the most efficient way you know to compute $c_1$?   How does the number of arithmetic operations scale with $n$?   How does this compare to solving $Xc=x$ for $c$ by Gaussian elimination?  Which would you choose?\n",
    "\n",
    "**(c)** Explain why you can write $X^{-1}$ as the product of $X^T$ and a *diagonal* matrix.  What are the entries of the diagonal matrix, and does it multiply $X^T$ on the left or right?\n",
    "\n",
    "**(d)** Check your answer from (c) in Julia for the following 4 orthogonal vectors.  Note that you can form a diagonal matrix with diagonal entries $a,b,c,d$ by `Diagonal([a,b,c,d])` in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [1,1,1,1]\n",
    "x2 = [2,-2,1,-1]\n",
    "x3 = [1,1,-1,-1]\n",
    "x4 = [1,-1,-2,2]\n",
    "X = [x1 x2 x3 x4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X'X # check orthogonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X' multiplied on the left or right by Diagonal([...])\n",
    "# should match inv(X) above:\n",
    "\n",
    "...???..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.3",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
