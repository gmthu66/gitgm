{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 8 solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (10 points)\n",
    "\n",
    "**(a)** The following Julia code constructs 20 random orthogonal matrices $Q$ (a square matrix with orthonormal columns) and prints their determinants.  Try running it a few times.  **Explain what you observe** using the properties of determinants:\n",
    "\n",
    "(Hint: $Q^T = Q^{-1}$.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** The following code generates 20 random 5×5 \"anti-symmetric\" (or \"skew-symmetric\") matrices, and prints their determinants.  This is a square matrix $A$ with $A^T = -A$.  **Explain what you observe** using the properties of determinants.\n",
    "\n",
    "**(c)** What happens if you try the anti-symmetric experiment again, but change the size (`m`) to 6?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** We run the code, observing the output below.  We notice that we always get either +1.0 or -1.0, and each about half of the time.  This makes sense, because if $Q$ is orthogonal, then by definition it has $Q^TQ = I$.  Taking determinants of both sides, we get $\\det(Q^T)\\det(Q) = \\det(I)$, because the determinant of a product of square matrices is the product of the determinants.  But also taking transpose doesn't change the determinant, i.e. $\\det(Q^T) = \\det(Q)$, and the determinant of the identity matrix is 1, so this equation says $\\det(Q)^2 = 1$.  The only numbers whose squares are 1 are 1 and -1, so we must have $\\boxed{\\det(Q) = \\pm 1}$ when $Q$ is orthogonal.  Also, if $Q$ is a randomly chosen orthogonal matrix (whatever that means precisely), then a given matrix is just as likely to appear as the same matrix with the first column negated (which is still orthogonal); negating the first column will flip the sign of the determinant, so we should see $+1$ and -1 equally often, as we do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "-1.0\n",
      "1.0\n",
      "1.0\n",
      "-1.0\n",
      "-1.0\n",
      "1.0\n",
      "1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "-1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "m = 5  # you can try changing this too if you want\n",
    "\n",
    "for i = 1:20\n",
    "    A = randn(m,m) # a random m×m matrix\n",
    "    Q, R = qr(A)   # QR factorizing A gives a random Q\n",
    "    Q = Q * diagm(rand([-1,+1],m)) # choose signs randomly too\n",
    "\n",
    "    println(round(det(Q), 13)) # print determinant, rounded to 13 digits\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** We run the given code, and the output is below.  We observe that we always get 0.  To see why, suppose $A$ is an anti-symmetric $5 \\times 5$ matrix, i.e. $A^T = -A$.  Taking determinants, this says $\\det(A^T) = \\det(-A)$.  We know $\\det(A^T) = \\det(A)$.  But negating $A$ negates all 5 columns of $A$, which multiplies the determinant by $(-1)^5 = -1$.  So $\\det(-A) = -\\det(A)$ **because $A$ has an odd number of columns**.  But then we get $\\det(A) = - \\det(A)$, which can only happen if $\\boxed{\\det(A) = 0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "-0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-0.0\n",
      "-0.0\n",
      "0.0\n",
      "-0.0\n",
      "0.0\n",
      "-0.0\n",
      "0.0\n",
      "-0.0\n",
      "-0.0\n",
      "-0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "m = 5  # you can try changing this too if you want\n",
    "\n",
    "for i = 1:20\n",
    "    A = randn(m,m) # a random m×m matrix\n",
    "    A = A - A' # make it skew-symmetric by subtracting its transpose\n",
    "\n",
    "    println(round(det(A), 13)) # print determinant, rounded to 13 digits\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** We changed $m$ to 6 and got the output below - a bunch of random-looking positive numbers! (The zero matrix is also anti-symmetric and has determinant exactly 0, for example, but a random anti-symmetric matrix will not have determinant zero.)  Notice that in the argument in part **(b)**, we would now get $\\det(A) = (-1)^6\\det(A)$, which doesn't say anything at all about $\\det(A)$.\n",
    "\n",
    "Why were the numbers all positive?  This is a mystery that we will have to unravel on a future pset, once we've learned that real-symmetric and anti-symmetric matrices have eigenvalues with special properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.6184399579619\n",
      "4.5839135916955\n",
      "4.9364701966096\n",
      "0.8268312810651\n",
      "1.8345011784043\n",
      "228.7530815218929\n",
      "118.3288192183671\n",
      "48.5246887082078\n",
      "294.7421647517407\n",
      "6.4987272876552\n",
      "164.688887329974\n",
      "7.3934614387992\n",
      "53.5479677891201\n",
      "446.2195345777764\n",
      "226.4321814353667\n",
      "9.0633128537034\n",
      "3.6025616779741\n",
      "785.3750863126804\n",
      "4.0856359848337\n",
      "10.6241907647929\n"
     ]
    }
   ],
   "source": [
    "m = 6  # you can try changing this too if you want\n",
    "\n",
    "for i = 1:20\n",
    "    A = randn(m,m) # a random m×m matrix\n",
    "    A = A - A' # make it skew-symmetric by subtracting its transpose\n",
    "\n",
    "    println(round(det(A), 13)) # print determinant, rounded to 13 digits\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (10 points)\n",
    "\n",
    "**(a)** Find the eigenvalues and eigenvectors of\n",
    "$$\n",
    "A = \\begin{pmatrix} 1 & 4 \\\\ 2 & 3 \\end{pmatrix}, \\qquad A + 2I = \\begin{pmatrix} 3 & 4 \\\\ 2 & 5 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**(b)** If $Ax=\\lambda x$, then what is $(A+\\alpha I)x$?   Therefore, how are the eigenvalues and eigenvectors of $A+\\alpha I$ related to those of $A$?   Is this consistent with your answer from (a)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** Let's compute the characteristic polynomial: $$p_A(\\lambda) = \\det(A - \\lambda I) = \\det(\\begin{pmatrix}1 - \\lambda & 4 \\\\ 2 & 3 - \\lambda\\end{pmatrix} = (1 - \\lambda)(3 - \\lambda) - 8 = \\lambda^2 - 4\\lambda - 5 = (\\lambda - 5)(\\lambda + 1).$$  This has roots at $\\boxed{\\lambda = 5}$ and $\\boxed{\\lambda = -1}$, so these are the eigenvalues.  To find an eigenvector with eigenvalue 5, we need to find a nonzero vector $(x, y)$ such that $$\\begin{pmatrix}1 & 4 \\\\ 2 & 3 \\end{pmatrix}\\begin{pmatrix} x & y\\end{pmatrix} = \\begin{pmatrix} x + 4y \\\\ 2x + 3y\\end{pmatrix} = \\begin{pmatrix} 5x \\\\ 5y\\end{pmatrix}.$$  Eyeballing it, we see that $\\boxed{(1, 1) \\text{ is an eigenvector with eigenvalue 5}}$ (and so is any nonzero multiple of it).   We could also compute this more \"mechanically\" by finding the nullspace of $A-5I = \\begin{pmatrix} -4 & 4 \\\\ 2 & -2 \\end{pmatrix}$, but the nullspace vector (the eigenvector!) $(1,1)$ is even more obvious from this matrix.  For eigenvalue -1, we need to find some nonzero $(x, y)$ such that $$\\begin{pmatrix}1 & 4 \\\\ 2 & 3 \\end{pmatrix}\\begin{pmatrix} x & y\\end{pmatrix} = \\begin{pmatrix} x + 4y \\\\ 2x + 3y\\end{pmatrix} = \\begin{pmatrix} -x \\\\ -y\\end{pmatrix}.$$  Eyeballing it, we see that $\\boxed{(-2, 1) \\text{ is an eigenvector with eigenvalue -1}}$ (and so is any nonzero multiple of it).  Again, the more systematic procedure is to find $N(A+I)$, but since $A+I = \\begin{pmatrix} 2 & 4 \\\\ 2 & 4 \\end{pmatrix}$ the nullspace vector $(-2, 1)$ should be obvious.\n",
    "\n",
    "For $A+2I$, the characteristic polynomial is $(3-\\lambda)(5-\\lambda) - 8 = \\lambda^2 - 8\\lambda + 7 = (\\lambda - 7) (\\lambda - 1)$.   So the eigenvalues are **λ=7** and **λ=1**: the old eigenvalues **plus 2**.   If we find the eigenvectors, they are the **same** as before, since they are the nullspaces of $A+2I-7I=A-5I$ and $A+2I-I=A+I$, the same as above!\n",
    "\n",
    "**(b)** If $Ax = \\lambda x$ then $(A + \\alpha I)x = Ax + \\alpha Ix = \\lambda x + \\alpha x = \\boxed{(\\lambda + \\alpha)x}$.   That is **A+αI has the same eigenvectors as A**, and the eigenvalues λ of A become **eigenvalues λ+α of A+αI**. This agrees with part (a), taking $\\alpha = 2$ (we have 7=5+2 and 1=–1+2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (10 points)\n",
    "\n",
    "(From Strang, section 6.1, problem 11.)\n",
    "\n",
    "Here is a strange fact about 2×2 matrices with eigenvalues $\\lambda_1 \\ne \\lambda_2$: the columns of $A - \\lambda_1 I$ are multiples of the eigenvector $x_2$.  *(You can see this if you look at the first 2×2 example from the [lecture-20 notebook](http://nbviewer.jupyter.org/github/stevengj/1806/blob/spring17/lectures/Eigenvalue-Polynomials.ipynb#Eigenvectors).)*   **Why?**\n",
    "\n",
    "(Hint: think about the column and null spaces of $A - \\lambda_1 I$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Let's think about what $A - \\lambda_1 I$ does to an arbitrary vector $x$.   If we write $x = c_1 x_1 + c_2 x_2$ in the basis of the two eigenvectors of $A$ (which must be diagonalizable since the eigenvalues are distinct), then $Ax = \\lambda_1 c_1 x_1 + \\lambda_2 c_2 x_2$, and \n",
    "$$\n",
    "(A - \\lambda_1 I) x = (\\lambda_2 - \\lambda_1) c_2 x_2\n",
    "$$\n",
    "That is, the **outputs are always a multiple of x₂**!   This means that $C(A - \\lambda_1 I)$ is spanned by $x₂$: it is a **rank 1 matrix**, and so every column must be a multiple of **x₂**!\n",
    "\n",
    "Another way of seeing this is that the *whole point* of $A - \\lambda_1 I$ is to have a nullspace containing the eigenvector $x_1$.  Since the $\\lambda_2 \\ne \\lambda_1$, the nullspace does not contain $x_2$, so it is a 1d nullspace and the matrix is rank-1.  Acting on the other eigenvector, it gives a multiple of that eigenvector, so $x_2$ must span the 1d column space exactly as above.\n",
    "\n",
    "Another way of seeing this is that, from problem 2, $A - \\lambda_1 I$ has the same eigenvectors as A, but with $\\lambda_1$ subtracted from the eigenvalues.  So it has eigenvalues of **zero** and $\\lambda_2-\\lambda_1$: the former gives the nullspace and the latter gives the column space.\n",
    "\n",
    "The moral of this story: once you have a basis of eigenvectors, in order to understand what the matrix does, **always look at what it does to the eigenvectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (10 points)\n",
    "\n",
    "In class, we saw that taking a random vector $x$ and repeatedly multiplying by $A$ (i.e. computing $A^n x$ for a large $n$) quickly gives something proportional to the *eigenvector with the largest |λ|*.  This happens for any $m\\times m$ matrix with $m$ eigenvalues of distinct magnitudes.\n",
    "\n",
    "**(a)** How could you modify this process to instead obtain the *eigenvector with the smallest |λ|*?\n",
    "\n",
    "You can assume that $A$ is nonsingular.  If $A$ is singular, why is there an easy way to find the eigenvector with the smallest |λ|?\n",
    "\n",
    "**(b)** The code below applies the repeated-multiplication process to the [2×2 example from lecture 20](http://nbviewer.jupyter.org/github/stevengj/1806/blob/spring17/lectures/Eigenvalue-Polynomials.ipynb#Eigenvalue-example:).   Run it to see it converging to the largest-|λ| eigenvector.  Then modify the code to your answer from (a) and run it again to verify that it converges to the smallest-|λ| eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 2.0\n",
       " 3.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [ 1 1 \n",
    "     -2 4]\n",
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, -5.27273]\n",
      "[1.0, 5.40426]\n",
      "[1.0, 3.06312]\n",
      "[1.0, 2.5233]\n",
      "[1.0, 2.29705]\n",
      "[1.0, 2.18019]\n",
      "[1.0, 2.11332]\n",
      "[1.0, 2.0728]\n",
      "[1.0, 2.04738]\n",
      "[1.0, 2.0311]\n",
      "[1.0, 2.02052]\n",
      "[1.0, 2.01359]\n",
      "[1.0, 2.00902]\n",
      "[1.0, 2.00599]\n",
      "[1.0, 2.00399]\n",
      "[1.0, 2.00265]\n",
      "[1.0, 2.00177]\n",
      "[1.0, 2.00118]\n",
      "[1.0, 2.00079]\n",
      "[1.0, 2.00052]\n"
     ]
    }
   ],
   "source": [
    "x = [-17, 6] # a random vector\n",
    "for i = 1:20\n",
    "    x = A*x\n",
    "    x = x / x[1] # normalize it to make x[1]=1, for comparison with lecture\n",
    "    println(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It converges to (1,2), the eigenvector for λ=3.  Your modified code should converge instead to (1,1), the eigenvector for λ=2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** Let $\\lambda_k$ be the eigenvalues of $A$.  If $A$ is nonsingular, then all $\\lambda_k \\ne 0$  eigenvalues of $A^{-1}$ are the reciprocals, $1/\\lambda_k$.  The eigenvectors of $A^{-1}$ are the *same* as those of $A$.\n",
    "\n",
    "(From class: if we have $Ax=\\lambda x$, then multiplying both sides by $A^{-1}$ and dividing by λ gives $A^{-1}x=\\lambda^{-1} x$.)\n",
    "\n",
    "So, **repeatedly multiplying by** $A^{-1}$ you can find an eigenvector for $A^{-1}$ with largest $|1/\\lambda_k|$, which is the same as the eigenvector for $A$ with *smallest* $|\\lambda_k|$.\n",
    "\n",
    "If, in the course of inverting (or LU factorizing) $A$, we find that it is singular, then the solution is even easier, no iteration/convergence required: the smallest $|\\lambda|$ is just $\\lambda=0$, and the corresponding eigenvector is just a basis for $N(A)$.\n",
    "\n",
    "**(b)** The code for this part is written and run below.  It converges to (1, 1), the eigenvector for $\\lambda = 2$, as promised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.378378]\n",
      "[1.0, 0.656716]\n",
      "[1.0, 0.794643]\n",
      "[1.0, 0.871866]\n",
      "[1.0, 0.918077]\n",
      "[1.0, 0.946836]\n",
      "[1.0, 0.965175]\n",
      "[1.0, 0.977049]\n",
      "[1.0, 0.984816]\n",
      "[1.0, 0.989928]\n",
      "[1.0, 0.993308]\n",
      "[1.0, 0.995549]\n",
      "[1.0, 0.997037]\n",
      "[1.0, 0.998026]\n",
      "[1.0, 0.998685]\n",
      "[1.0, 0.999124]\n",
      "[1.0, 0.999416]\n",
      "[1.0, 0.999611]\n",
      "[1.0, 0.999741]\n",
      "[1.0, 0.999827]\n"
     ]
    }
   ],
   "source": [
    "x = [-17, 6] # a random vector\n",
    "B = lufact(A) # could also use B=inv(A), but it's almost always better to work with the LU factorization\n",
    "for i = 1:20\n",
    "    x = B \\ x # could also do A \\ x here, or even inv(A)*x, but it's wasteful to re-do elimination each time\n",
    "    x = x / x[1] # normalize it to make x[1]=1, for comparison with lecture\n",
    "    println(x)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
